---
title: "LangChainê³¼ LangGraph ì™„ë²½ ê°€ì´ë“œ: AI ì—ì´ì „íŠ¸ ê°œë°œì˜ ëª¨ë“  ê²ƒ"
date: 2025-10-27T23:55:34+09:00
draft: false
tags: ["LangChain", "LangGraph", "AI Agent", "RAG", "LLM", "Python"]
categories: ["AI"]
description: "LangChainê³¼ LangGraphì˜ í•µì‹¬ ê°œë…ë¶€í„° ì‹¤ì „ í™œìš©ê¹Œì§€, AI ì—ì´ì „íŠ¸ ê°œë°œì„ ìœ„í•œ ì™„ë²½ ê°€ì´ë“œ. RAG, ì—ì´ì „íŠ¸ íŒ¨í„´, Unity ê²Œì„ ì ìš© ì‚¬ë¡€ í¬í•¨"
---

## ğŸ¯ Part 1: LangChain ì†Œê°œ

### LangChainì´ë€

LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬. ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ ì²´ì´ë‹ íŒ¨í„´ìœ¼ë¡œ ê°„ì†Œí™”

**í•µì‹¬ íŠ¹ì§•:**
- **ì‹¤ì‹œê°„ ë°ì´í„° ì¦ê°•**: RAGë¥¼ í†µí•œ ìµœì‹  ì •ë³´ í™œìš©
- **ëª¨ë¸ ìƒí˜¸ ìš´ìš©ì„±**: OpenAI, Anthropic, Cohere ë“± ë‹¤ì–‘í•œ LLM ì§€ì›
- **ëª¨ë“ˆí™”ëœ êµ¬ì„±**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ ì¡°í•©

### ì™œ LangChainì¸ê°€

```python
# LangChain ì—†ì´ (vanilla code)
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)
result = response.choices[0].message.content

# LangChain ì‚¬ìš©
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

chain = ChatPromptTemplate.from_template("Translate {text} to {language}") | ChatOpenAI(model="gpt-4")
result = chain.invoke({"text": "Hello", "language": "Korean"})
```

**ì¥ì :**
- âœ… ì½”ë“œ ê°„ê²°í™” (ì²´ì´ë‹ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ)
- âœ… ëª¨ë¸ êµì²´ ìš©ì´ (ì¶”ìƒí™” ê³„ì¸µ ì œê³µ)
- âœ… í”„ë¡œë•ì…˜ ê¸°ëŠ¥ ë‚´ì¥ (ì¬ì‹œë„, ë¡œê¹…, ëª¨ë‹ˆí„°ë§)

### í•µì‹¬ ê°œë…

**1. ì²´ì´ë‹ (Chaining)**
```python
chain = prompt | model | output_parser
result = chain.invoke(input_data)
```

**2. ëª¨ë“ˆí™”**
- **Prompts**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- **Models**: LLM ì¶”ìƒí™” ê³„ì¸µ
- **Output Parsers**: êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹±
- **Retrievers**: ë²¡í„° DB ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤

**3. ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€**
- ğŸ” RAG (Retrieval-Augmented Generation)
- ğŸ’¬ ì±—ë´‡ ë° ëŒ€í™”í˜• ì‹œìŠ¤í…œ
- ğŸ“ ë¬¸ì„œ ìš”ì•½ ë° Q&A
- ğŸ¤– ì—ì´ì „íŠ¸ (ë„êµ¬ ì‚¬ìš©)

### LangChain ìƒíƒœê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangChain Ecosystem             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LangChain Core                          â”‚
â”‚ - ê¸°ë³¸ ì¶”ìƒí™” (Chains, Prompts, Models) â”‚
â”‚                                         â”‚
â”‚ LangSmith                               â”‚
â”‚ - ë””ë²„ê¹…, ëª¨ë‹ˆí„°ë§, í‰ê°€ í”Œë«í¼          â”‚
â”‚                                         â”‚
â”‚ LangGraph                               â”‚
â”‚ - ë³µì¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°        â”‚
â”‚                                         â”‚
â”‚ LangGraph Platform                      â”‚
â”‚ - LangGraph ì•± ë°°í¬ ë° í˜¸ìŠ¤íŒ…           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ê¸°ë³¸ ì˜ˆì œ: Simple LLM Chain

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful translator."),
    ("user", "Translate '{text}' to {language}")
])

# 2. ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOpenAI(model="gpt-4", temperature=0)

# 3. ì¶œë ¥ íŒŒì„œ
parser = StrOutputParser()

# 4. ì²´ì¸ êµ¬ì„±
translation_chain = prompt | model | parser

# 5. ì‹¤í–‰
result = translation_chain.invoke({
    "text": "Hello, world!",
    "language": "Korean"
})
print(result)  # "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ìƒ!"
```

---

## ğŸ—ï¸ Part 2: LangChain ì•„í‚¤í…ì²˜

### 1. Basic LLM Chain êµ¬ì¡°

```
Input
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Templateâ”‚
â”‚  (ë³€ìˆ˜ ì‚½ì…)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Model     â”‚
â”‚  (ì¶”ë¡  ì‹¤í–‰)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Parser  â”‚
â”‚  (ê²°ê³¼ íŒŒì‹±)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
      Output
```

**ì½”ë“œ êµ¬í˜„:**
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ì²´ì¸ êµ¬ì„± (LCEL: LangChain Expression Language)
chain = (
    ChatPromptTemplate.from_template("Tell me a joke about {topic}")
    | ChatOpenAI(model="gpt-4")
    | StrOutputParser()
)

# ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
for chunk in chain.stream({"topic": "programming"}):
    print(chunk, end="", flush=True)
```

### 2. RAG (Retrieval-Augmented Generation) ì•„í‚¤í…ì²˜

RAGëŠ” ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±:

**Phase 1: Indexing (ì˜¤í”„ë¼ì¸)**
```
Documents
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document Loader â”‚  â† PDF, HTML, DB ë“±
â”‚  (ë¬¸ì„œ ë¡œë”©)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Text Splitter  â”‚  â† RecursiveCharacterTextSplitter
â”‚   (ì²­í¬ ë¶„í• )     â”‚     chunk_size=1000, overlap=200
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Model â”‚  â† OpenAI, Cohere ë“±
â”‚  (ë²¡í„° ë³€í™˜)      â”‚     text â†’ [0.1, 0.3, ...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Store   â”‚  â† Chroma, Pinecone, FAISS
â”‚  (ë²¡í„° DB ì €ì¥)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 2: Retrieval + Generation (ì˜¨ë¼ì¸)**
```
User Query: "Unityì—ì„œ AI ì—ì´ì „íŠ¸ êµ¬í˜„ ë°©ë²•?"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Embedding â”‚  â† ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
â”‚                  â”‚     "Unity AI..." â†’ [0.2, 0.5, ...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Similarity      â”‚  â† ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
â”‚  Search (Top-K)  â”‚     ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ Kê°œ ì²­í¬ ë°˜í™˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context + Query â”‚  â† Retrieved docs + Original query
â”‚  (í”„ë¡¬í”„íŠ¸ êµ¬ì„±)  â”‚     "Based on: [docs], answer: [query]"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Generate   â”‚  â† GPT-4ë¡œ ë‹µë³€ ìƒì„±
â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
       Answer
```

**RAG ì „ì²´ ì½”ë“œ:**
```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# === Phase 1: Indexing ===
# 1. ë¬¸ì„œ ë¡œë”©
loader = PyPDFLoader("unity_documentation.pdf")
docs = loader.load()

# 2. í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings()
)

# === Phase 2: Retrieval + Generation ===
# 4. Retriever ìƒì„±
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer using the following context:\n\n{context}"),
    ("user", "{input}")
])

# 6. ì²´ì¸ êµ¬ì„±
llm = ChatOpenAI(model="gpt-4")
document_chain = create_stuff_documents_chain(llm, prompt)
retrieval_chain = create_retrieval_chain(retriever, document_chain)

# 7. ì¿¼ë¦¬ ì‹¤í–‰
response = retrieval_chain.invoke({
    "input": "Unityì—ì„œ NavMeshë¥¼ ì‚¬ìš©í•œ AI ì´ë™ êµ¬í˜„ ë°©ë²•?"
})
print(response["answer"])
```

### 3. Agent ì•„í‚¤í…ì²˜

ììœ¨ì  ì˜ì‚¬ê²°ì • ë£¨í”„:

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Agent Loop                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Think (ë‹¤ìŒ í–‰ë™ ê²°ì •)     â”‚  â”‚
â”‚  â”‚    LLM: "ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© í•„ìš”" â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. Act (ë„êµ¬ ì‹¤í–‰)           â”‚  â”‚
â”‚  â”‚    Tool: search("Unity AI")  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. Observe (ê²°ê³¼ ë¶„ì„)       â”‚  â”‚
â”‚  â”‚    Result: [search results]  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚        ì¶©ë¶„í•œê°€?                    â”‚
â”‚       /        \                    â”‚
â”‚     Yes        No â”€â”€â”               â”‚
â”‚      â”‚              â”‚               â”‚
â”‚      â–¼              â”‚               â”‚
â”‚   Final Answer â—„â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Agent ì½”ë“œ:**
```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools import DuckDuckGoSearchRun

# 1. ë„êµ¬ ì •ì˜
tools = [
    DuckDuckGoSearchRun(name="web_search"),
]

# 2. í”„ë¡¬í”„íŠ¸ (agent_scratchpad í•„ìˆ˜)
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

# 3. Agent ìƒì„±
llm = ChatOpenAI(model="gpt-4", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)

# 4. Executorë¡œ ë˜í•‘ (ë£¨í”„ ì‹¤í–‰ ë‹´ë‹¹)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# 5. ì‹¤í–‰
result = agent_executor.invoke({
    "input": "Unity 2023 LTSì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ì€?"
})
print(result["output"])
```

---

## ğŸ”€ Part 3: LangGraph ì†Œê°œ

### LangGraphë€

ê·¸ë˜í”„ ê¸°ë°˜ìœ¼ë¡œ ë³µì¡í•œ AI ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” í”„ë ˆì„ì›Œí¬. **ìˆœí™˜ ê·¸ë˜í”„(cyclic graph)**ë¡œ ìƒíƒœ ë¨¸ì‹  êµ¬í˜„

**í•µì‹¬ íŠ¹ì§•:**
- **ìˆœí™˜ ì›Œí¬í”Œë¡œìš°**: ë£¨í”„, ì¡°ê±´ë¶€ ë¶„ê¸°, ì¬ì‹œë„ ë¡œì§
- **ëª…ì‹œì  ìƒíƒœ ê´€ë¦¬**: TypedDictë¡œ ìƒíƒœ ì¶”ì 
- **ì²´í¬í¬ì¸íŠ¸**: ì‹¤í–‰ ì¤‘ ìƒíƒœ ì €ì¥ ë° ë³µì›
- **Human-in-the-Loop**: ì‚¬ëŒì˜ ìŠ¹ì¸/ìˆ˜ì • ì§€ì  ì‚½ì…

### LangChain vs LangGraph ë¹„êµ

| íŠ¹ì„± | LangChain | LangGraph |
|------|-----------|-----------|
| **êµ¬ì¡°** | ì„ í˜• ì²´ì¸ (Linear) | ìˆœí™˜ ê·¸ë˜í”„ (Cyclic) |
| **ìƒíƒœ ê´€ë¦¬** | ì•”ì‹œì  (ì²´ì¸ ê°„ ì „ë‹¬) | ëª…ì‹œì  (TypedDict) |
| **ì¡°ê±´ë¶€ ë¡œì§** | ê°„ë‹¨í•œ ë¶„ê¸° | ë³µì¡í•œ ì¡°ê±´ë¶€ íë¦„ |
| **ë£¨í”„** | ì œí•œì  | ë„¤ì´í‹°ë¸Œ ì§€ì› |
| **ì²´í¬í¬ì¸íŠ¸** | ì—†ìŒ | ë‚´ì¥ (ì‹œê°„ ì—¬í–‰ ê°€ëŠ¥) |
| **Human-in-the-Loop** | ìˆ˜ë™ êµ¬í˜„ í•„ìš” | ë‚´ì¥ ê¸°ëŠ¥ |
| **ì‚¬ìš© ì‚¬ë¡€** | RAG, ë‹¨ìˆœ ì²´ì¸ | ë©€í‹° ì—ì´ì „íŠ¸, ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° |
| **ì½”ë“œ ë³µì¡ë„** | ë‚®ìŒ | ì¤‘ê°„~ë†’ìŒ |

**ì˜ˆì‹œ: LangChainì€ í­í¬ìˆ˜, LangGraphëŠ” ë¯¸ë¡œ**

```
LangChain:  A â†’ B â†’ C â†’ D (ì§ì„ )

LangGraph:  A â†’ B â‡„ C â†’ D
              â†“     â†‘
              E â”€â”€â”€â”€â”˜
            (ìˆœí™˜, ì¬ì‹œë„, ì¡°ê±´ë¶€)
```

### ì–¸ì œ LangGraphë¥¼ ì‚¬ìš©í• ê¹Œ

**LangGraph ì í•©:**
- âœ… ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… (Agent Aê°€ Agent B í˜¸ì¶œ)
- âœ… ì¬ì‹œë„/ê²€ì¦ ë¡œì§ (ë‹µë³€ ê²€ì¦ í›„ ì¬ìƒì„±)
- âœ… Human-in-the-Loop (ì‚¬ëŒ ìŠ¹ì¸ í•„ìš”)
- âœ… ë³µì¡í•œ ìƒíƒœ ì¶”ì  (ëŒ€í™” íˆìŠ¤í† ë¦¬, ì»¨í…ìŠ¤íŠ¸)

**LangChain ì í•©:**
- âœ… ë‹¨ìˆœ RAG íŒŒì´í”„ë¼ì¸
- âœ… ì¼íšŒì„± ì²´ì¸ (ë²ˆì—­, ìš”ì•½)
- âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

---

## ğŸ§© Part 4: LangGraph í•µì‹¬ ê°œë…

### 1. State (ìƒíƒœ)

TypedDictë¡œ ì •ì˜ëœ ê³µìœ  ë°ì´í„°:

```python
from typing import TypedDict, Annotated
from langgraph.graph import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]  # ë©”ì‹œì§€ ëˆ„ì 
    context: str                              # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
    retry_count: int                          # ì¬ì‹œë„ íšŸìˆ˜
```

**Reducer í•¨ìˆ˜ (add_messages):**
- ê¸°ë³¸ ë™ì‘: ë®ì–´ì“°ê¸° (`state["key"] = value`)
- Reducer: ì»¤ìŠ¤í…€ ì—…ë°ì´íŠ¸ ë¡œì§ (`add_messages`ëŠ” ë¦¬ìŠ¤íŠ¸ì— append)

### 2. Nodes (ë…¸ë“œ)

ì‘ì—… ë‹¨ìœ„ (í•¨ìˆ˜ ë˜ëŠ” Runnable):

```python
def node_function(state: State) -> dict:
    """ë…¸ë“œëŠ” Stateë¥¼ ë°›ì•„ ì—…ë°ì´íŠ¸í•  dict ë°˜í™˜"""
    messages = state["messages"]
    # ì‘ì—… ìˆ˜í–‰
    response = llm.invoke(messages)
    return {"messages": [response]}
```

**íŠ¹ì§•:**
- ìˆœìˆ˜ í•¨ìˆ˜ (side-effect ìµœì†Œí™”)
- State ì½ê¸° ë° ì—…ë°ì´íŠ¸
- ë™ê¸°/ë¹„ë™ê¸° ëª¨ë‘ ì§€ì›

### 3. Edges (ì—£ì§€)

ë…¸ë“œ ê°„ ì—°ê²°:

**Normal Edge (ë¬´ì¡°ê±´ ì‹¤í–‰):**
```python
graph.add_edge("node_a", "node_b")  # A ì‹¤í–‰ í›„ í•­ìƒ B
```

**Conditional Edge (ì¡°ê±´ë¶€):**
```python
def router(state: State) -> str:
    """ë‹¤ìŒ ë…¸ë“œ ì´ë¦„ì„ ë°˜í™˜"""
    if state["retry_count"] > 3:
        return "end"
    return "retry"

graph.add_conditional_edges(
    "validation",
    router,
    {"retry": "generation", "end": END}
)
```

### 4. Graph Builder íŒ¨í„´

```python
from langgraph.graph import StateGraph, END

# 1. ê·¸ë˜í”„ ì •ì˜
graph = StateGraph(State)

# 2. ë…¸ë“œ ì¶”ê°€
graph.add_node("input", input_node)
graph.add_node("process", process_node)
graph.add_node("output", output_node)

# 3. ì—£ì§€ ì¶”ê°€
graph.set_entry_point("input")
graph.add_edge("input", "process")
graph.add_conditional_edges("process", router)
graph.add_edge("output", END)

# 4. ì»´íŒŒì¼
app = graph.compile()

# 5. ì‹¤í–‰
result = app.invoke({"messages": [("user", "Hello")]})
```

### 5. ì‹¤í–‰ ê¸°ëŠ¥

**Streaming (ì‹¤ì‹œê°„ ì¶œë ¥):**
```python
for event in app.stream({"messages": [("user", "Hello")]}):
    print(event)
```

**Checkpointing (ìƒíƒœ ì €ì¥):**
```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = graph.compile(checkpointer=memory)

# ìŠ¤ë ˆë“œë³„ ìƒíƒœ ìœ ì§€
config = {"configurable": {"thread_id": "thread-1"}}
app.invoke(input, config=config)
```

**Human-in-the-Loop:**
```python
from langgraph.checkpoint.memory import MemorySaver

app = graph.compile(checkpointer=MemorySaver(), interrupt_before=["human_review"])

# ì‹¤í–‰ì´ human_review ì „ì— ì¤‘ë‹¨
result = app.invoke(input, config=config)

# ì‚¬ëŒì´ ìŠ¹ì¸ í›„ ì¬ê°œ
app.invoke(None, config=config)
```

**Time-Travel (ê³¼ê±° ìƒíƒœë¡œ ë³µê·€):**
```python
# ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ
checkpoints = app.get_state_history(config)

# íŠ¹ checkpointë¡œ ë³µê·€
past_config = {"configurable": {"thread_id": "thread-1", "checkpoint_id": "abc123"}}
app.invoke(None, config=past_config)
```

---

## ğŸ¨ Part 5: LangGraph ì›Œí¬í”Œë¡œìš° íŒ¨í„´

### 1. Prompt Chaining (ìˆœì°¨ ì²˜ë¦¬)

```
Input â†’ Step1 â†’ Step2 â†’ Step3 â†’ Output
```

**ASCII ë‹¤ì´ì–´ê·¸ë¨:**
```
   START
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step1  â”‚  Outline ìƒì„±
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step2  â”‚  Draft ì‘ì„±
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step3  â”‚  Refine
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
    END
```

**ì½”ë“œ:**
```python
from langgraph.graph import StateGraph, END

class State(TypedDict):
    input: str
    outline: str
    draft: str
    final: str

def step1(state):
    outline = llm.invoke(f"Create outline for: {state['input']}")
    return {"outline": outline}

def step2(state):
    draft = llm.invoke(f"Write draft based on: {state['outline']}")
    return {"draft": draft}

def step3(state):
    final = llm.invoke(f"Refine: {state['draft']}")
    return {"final": final}

graph = StateGraph(State)
graph.add_node("step1", step1)
graph.add_node("step2", step2)
graph.add_node("step3", step3)
graph.set_entry_point("step1")
graph.add_edge("step1", "step2")
graph.add_edge("step2", "step3")
graph.add_edge("step3", END)

app = graph.compile()
```

### 2. Parallelization (ë³‘ë ¬ ì²˜ë¦¬)

```
         â”Œâ”€ Task1 â”€â”
Input â”€â”€â”¼â”€ Task2 â”€â”€â”¼â”€ Aggregate â†’ Output
         â””â”€ Task3 â”€â”˜
```

**ASCII ë‹¤ì´ì–´ê·¸ë¨:**
```
       START
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Fanout â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
    â”‚    â”‚    â”‚
    â–¼    â–¼    â–¼
 â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”
 â”‚ T1â”‚â”‚ T2â”‚â”‚ T3â”‚  (ë³‘ë ¬ ì‹¤í–‰)
 â””â”€â”¬â”€â”˜â””â”€â”¬â”€â”˜â””â”€â”¬â”€â”˜
   â”‚    â”‚    â”‚
   â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Fanin  â”‚  (ê²°ê³¼ ë³‘í•©)
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
       END
```

**ì½”ë“œ:**
```python
from langgraph.graph import StateGraph, END
from langgraph.constants import Send

class State(TypedDict):
    query: str
    results: list[str]

def parallel_search(state):
    # Send APIë¡œ ë³‘ë ¬ ë…¸ë“œ ìƒì„±
    return [
        Send("search_web", {"query": state["query"]}),
        Send("search_db", {"query": state["query"]}),
        Send("search_docs", {"query": state["query"]})
    ]

def search_web(state):
    result = web_search(state["query"])
    return {"results": [result]}

def search_db(state):
    result = db_query(state["query"])
    return {"results": [result]}

def search_docs(state):
    result = doc_search(state["query"])
    return {"results": [result]}

def aggregate(state):
    combined = "\n".join(state["results"])
    return {"final": combined}

graph = StateGraph(State)
graph.add_node("fanout", parallel_search)
graph.add_node("search_web", search_web)
graph.add_node("search_db", search_db)
graph.add_node("search_docs", search_docs)
graph.add_node("aggregate", aggregate)

graph.set_entry_point("fanout")
graph.add_edge("search_web", "aggregate")
graph.add_edge("search_db", "aggregate")
graph.add_edge("search_docs", "aggregate")
graph.add_edge("aggregate", END)

app = graph.compile()
```

### 3. Routing (ë¶„ë¥˜ ë° ë¼ìš°íŒ…)

```
         â”Œâ”€ Route A
Input â”€â”€Router
         â””â”€ Route B
```

**ASCII ë‹¤ì´ì–´ê·¸ë¨:**
```
      START
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Router  â”‚  (ì¿¼ë¦¬ ë¶„ë¥˜)
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚
    â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Math  â”‚ â”‚Generalâ”‚
â”‚Solver â”‚ â”‚  QA   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚       â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜
        â”‚
        â–¼
       END
```

**ì½”ë“œ:**
```python
def router(state):
    query = state["query"]
    classification = llm.invoke(f"Classify query: {query}. Reply 'math' or 'general'")
    if "math" in classification.lower():
        return "math"
    return "general"

def math_solver(state):
    answer = solve_math(state["query"])
    return {"answer": answer}

def general_qa(state):
    answer = llm.invoke(state["query"])
    return {"answer": answer}

graph = StateGraph(State)
graph.add_node("router", lambda s: s)
graph.add_node("math", math_solver)
graph.add_node("general", general_qa)

graph.set_entry_point("router")
graph.add_conditional_edges(
    "router",
    router,
    {"math": "math", "general": "general"}
)
graph.add_edge("math", END)
graph.add_edge("general", END)

app = graph.compile()
```

### 4. Orchestrator-Worker (ë™ì  ì‘ì—… ë¶„ë°°)

```
Orchestrator â†’ [Worker1, Worker2, ...] â†’ Orchestrator
     â†“                                         â†“
  ê³„íš ìˆ˜ë¦½                                  ê²°ê³¼ í†µí•©
```

**ASCII ë‹¤ì´ì–´ê·¸ë¨:**
```
       START
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Orchestrator â”‚  "3ê°œ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±"
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
     [Send API]
         â”‚
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
    â”‚    â”‚    â”‚
    â–¼    â–¼    â–¼
  â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”
  â”‚W1 â”‚â”‚W2 â”‚â”‚W3 â”‚  (ë™ì  ìƒì„±)
  â””â”€â”¬â”€â”˜â””â”€â”¬â”€â”˜â””â”€â”¬â”€â”˜
    â”‚    â”‚    â”‚
    â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Orchestrator â”‚  ê²°ê³¼ í†µí•©
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
        END
```

**ì½”ë“œ:**
```python
def orchestrator(state):
    if "tasks" not in state:
        # ì´ˆê¸° ê³„íš
        plan = llm.invoke(f"Break down: {state['query']}")
        tasks = parse_tasks(plan)
        return {"tasks": tasks}
    else:
        # ê²°ê³¼ í†µí•©
        final = combine_results(state["worker_results"])
        return {"final": final}

def worker(state):
    task = state["current_task"]
    result = execute_task(task)
    return {"worker_results": [result]}

graph = StateGraph(State)
graph.add_node("orchestrator", orchestrator)
graph.add_node("worker", worker)

def route_after_orchestrator(state):
    if "tasks" in state and not state.get("worker_results"):
        # ì‘ì—…ìë“¤ì—ê²Œ ë¶„ë°°
        return [Send("worker", {"current_task": t}) for t in state["tasks"]]
    return "end"

graph.set_entry_point("orchestrator")
graph.add_conditional_edges("orchestrator", route_after_orchestrator)
graph.add_edge("worker", "orchestrator")

app = graph.compile()
```

### 5. Evaluator-Optimizer (ë°˜ë³µ ê°œì„ )

```
Generate â†’ Evaluate â†’ Refine â†’ Evaluate â†’ ...
    â†‘                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (ì¬ì‹œë„ ë£¨í”„)
```

**ASCII ë‹¤ì´ì–´ê·¸ë¨:**
```
      START
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Generate â”‚  ì´ˆê¸° ë‹µë³€ ìƒì„±
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Evaluate â”‚  í’ˆì§ˆ í‰ê°€
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
     Pass?
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   Yes     No
    â”‚       â”‚
    â”‚       â–¼
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â”‚ Refine  â”‚  ê°œì„ 
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚       â”‚
    â”‚       â””â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚
    â–¼              â–¼
   END      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Evaluate â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (ë£¨í”„)
```

**ì½”ë“œ:**
```python
class State(TypedDict):
    query: str
    answer: str
    score: float
    retry_count: int

def generate(state):
    answer = llm.invoke(state["query"])
    return {"answer": answer, "retry_count": state.get("retry_count", 0)}

def evaluate(state):
    score = llm.invoke(f"Rate quality (0-10): {state['answer']}")
    return {"score": float(score)}

def refine(state):
    improved = llm.invoke(f"Improve: {state['answer']}")
    return {
        "answer": improved,
        "retry_count": state["retry_count"] + 1
    }

def should_continue(state):
    if state["score"] >= 8 or state["retry_count"] >= 3:
        return "end"
    return "refine"

graph = StateGraph(State)
graph.add_node("generate", generate)
graph.add_node("evaluate", evaluate)
graph.add_node("refine", refine)

graph.set_entry_point("generate")
graph.add_edge("generate", "evaluate")
graph.add_conditional_edges(
    "evaluate",
    should_continue,
    {"refine": "refine", "end": END}
)
graph.add_edge("refine", "evaluate")

app = graph.compile()
```

### 6. Agent (ììœ¨ ì—ì´ì „íŠ¸)

```
Think â†’ Act â†’ Observe â†’ Think â†’ ...
  â†‘              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (ë„êµ¬ ì‚¬ìš© ë£¨í”„)
```

**ASCII ë‹¤ì´ì–´ê·¸ë¨:**
```
       START
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Agent  â”‚  "ë¬´ì—‡ì„ í• ê¹Œ?"
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
Finish?      No
    â”‚         â”‚
   Yes        â–¼
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    â”‚  Tool   â”‚  ê²€ìƒ‰, ê³„ì‚° ë“±
    â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚         â”‚
    â”‚         â–¼
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    â”‚  Agent  â”‚  "ê²°ê³¼ ë¶„ì„"
    â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                   â”‚
                   â–¼
                  END
```

**ì½”ë“œ:**
```python
from langchain.agents import create_openai_functions_agent
from langchain_core.messages import HumanMessage

class AgentState(TypedDict):
    messages: Annotated[list, add_messages]

def call_agent(state):
    response = agent_executor.invoke({"input": state["messages"][-1].content})
    return {"messages": [HumanMessage(content=response["output"])]}

def should_continue(state):
    last_message = state["messages"][-1]
    if "FINAL ANSWER" in last_message.content:
        return "end"
    return "continue"

graph = StateGraph(AgentState)
graph.add_node("agent", call_agent)
graph.set_entry_point("agent")
graph.add_conditional_edges(
    "agent",
    should_continue,
    {"continue": "agent", "end": END}
)

app = graph.compile()
```

---

## ğŸ’» Part 6: ì‹¤ì „ ì½”ë“œ ì˜ˆì œ

### ì˜ˆì œ 1: ê°„ë‹¨í•œ ì±—ë´‡ with ë£¨í”„

**ìš”êµ¬ì‚¬í•­:** ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€, ë¬´í•œ ëŒ€í™” ê°€ëŠ¥

```python
from typing import Annotated
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

# 1. State ì •ì˜
class ChatState(TypedDict):
    messages: Annotated[list, add_messages]

# 2. Chatbot ë…¸ë“œ
def chatbot(state: ChatState):
    llm = ChatOpenAI(model="gpt-4", temperature=0.7)
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

# 3. ê·¸ë˜í”„ êµ¬ì„±
graph = StateGraph(ChatState)
graph.add_node("chatbot", chatbot)
graph.set_entry_point("chatbot")
graph.add_edge("chatbot", END)

# 4. ë©”ëª¨ë¦¬ ì¶”ê°€ (ëŒ€í™” íˆìŠ¤í† ë¦¬)
memory = MemorySaver()
app = graph.compile(checkpointer=memory)

# 5. ì‹¤í–‰ (ìŠ¤ë ˆë“œë³„ ìƒíƒœ ìœ ì§€)
config = {"configurable": {"thread_id": "user-123"}}

# ì²« ë©”ì‹œì§€
response1 = app.invoke(
    {"messages": [HumanMessage(content="ì•ˆë…•? ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼")]},
    config=config
)
print(response1["messages"][-1].content)
# "ì•ˆë…•í•˜ì„¸ìš” ì² ìˆ˜ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

# ë‘ ë²ˆì§¸ ë©”ì‹œì§€ (ì´ì „ ëŒ€í™” ê¸°ì–µ)
response2 = app.invoke(
    {"messages": [HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?")]},
    config=config
)
print(response2["messages"][-1].content)
# "ì² ìˆ˜ë‹˜ì´ì„¸ìš”!"
```

**ì‹¤í–‰ íë¦„:**
```
â”Œâ”€ Invoke 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: "ì•ˆë…•? ë‚´ ì´ë¦„ì€ ì² ìˆ˜"â”‚
â”‚   â†“                         â”‚
â”‚ [Checkpoint Save]           â”‚
â”‚   â†“                         â”‚
â”‚ Output: "ì•ˆë…•í•˜ì„¸ìš” ì² ìˆ˜ë‹˜!"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Invoke 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Checkpoint Load]           â”‚
â”‚   â†“                         â”‚
â”‚ Input: "ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?"  â”‚
â”‚   â†“                         â”‚
â”‚ Context: ì´ì „ ëŒ€í™” í¬í•¨     â”‚
â”‚   â†“                         â”‚
â”‚ Output: "ì² ìˆ˜ë‹˜ì´ì„¸ìš”!"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì˜ˆì œ 2: RAG + Self-Correction Agent

**ìš”êµ¬ì‚¬í•­:** ë‹µë³€ ê²€ì¦ í›„ ë¶€ì¡±í•˜ë©´ ì¬ê²€ìƒ‰

```python
from typing import TypedDict, Annotated
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END, add_messages

# 1. State ì •ì˜
class RAGState(TypedDict):
    query: str
    documents: list[str]
    answer: str
    grade: str
    retry_count: int

# 2. Retriever ë…¸ë“œ
def retrieve(state: RAGState):
    vectorstore = Chroma(embedding_function=OpenAIEmbeddings())
    docs = vectorstore.similarity_search(state["query"], k=3)
    return {
        "documents": [doc.page_content for doc in docs],
        "retry_count": state.get("retry_count", 0)
    }

# 3. Generator ë…¸ë“œ
def generate(state: RAGState):
    context = "\n".join(state["documents"])
    prompt = f"Context:\n{context}\n\nQuestion: {state['query']}"

    llm = ChatOpenAI(model="gpt-4")
    answer = llm.invoke([HumanMessage(content=prompt)])
    return {"answer": answer.content}

# 4. Grader ë…¸ë“œ (ë‹µë³€ í‰ê°€)
def grade_answer(state: RAGState):
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"""
    Question: {state['query']}
    Answer: {state['answer']}

    Is this answer good? Reply only 'good' or 'bad'.
    """
    grade = llm.invoke([HumanMessage(content=prompt)]).content.lower()
    return {"grade": grade}

# 5. Refine ë…¸ë“œ (ì¬ê²€ìƒ‰)
def refine_query(state: RAGState):
    llm = ChatOpenAI(model="gpt-4")
    improved_query = llm.invoke([
        HumanMessage(content=f"Rephrase for better search: {state['query']}")
    ]).content
    return {
        "query": improved_query,
        "retry_count": state["retry_count"] + 1
    }

# 6. Router (ê³„ì† vs ì¢…ë£Œ)
def should_continue(state: RAGState):
    if state["grade"] == "good" or state["retry_count"] >= 2:
        return "end"
    return "refine"

# 7. ê·¸ë˜í”„ êµ¬ì„±
graph = StateGraph(RAGState)
graph.add_node("retrieve", retrieve)
graph.add_node("generate", generate)
graph.add_node("grade", grade_answer)
graph.add_node("refine", refine_query)

graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "generate")
graph.add_edge("generate", "grade")
graph.add_conditional_edges(
    "grade",
    should_continue,
    {"refine": "refine", "end": END}
)
graph.add_edge("refine", "retrieve")

app = graph.compile()

# 8. ì‹¤í–‰
result = app.invoke({
    "query": "Unityì—ì„œ NavMesh ì‚¬ìš©ë²•?",
    "retry_count": 0
})
print(result["answer"])
```

**ì‹¤í–‰ íë¦„ (Self-Correction):**
```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieve â”‚  docs = ["NavMesh basics", ...]
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate â”‚  answer = "NavMeshëŠ”..."
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grade   â”‚  grade = "bad" (ì •ë³´ ë¶€ì¡±)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
  "refine"
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Refine  â”‚  query = "Unity NavMesh ìƒì„¸ ê°€ì´ë“œ"
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieve â”‚  docs = [ë” ë‚˜ì€ ë¬¸ì„œë“¤]
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate â”‚  answer = "NavMeshëŠ” Unityì˜..."
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grade   â”‚  grade = "good"
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
    END
```

---

## ğŸš€ Part 7: ê³ ê¸‰ ê¸°ëŠ¥

### 1. Human-in-the-Loop

**ì‚¬ìš© ì‚¬ë¡€:** ì¤‘ìš” ê²°ì • ì „ ì‚¬ëŒ ìŠ¹ì¸ í•„ìš”

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END

class ApprovalState(TypedDict):
    request: str
    approval: str
    result: str

def generate_plan(state):
    plan = llm.invoke(f"Create plan for: {state['request']}")
    return {"approval": plan}

def execute_plan(state):
    result = execute(state["approval"])
    return {"result": result}

graph = StateGraph(ApprovalState)
graph.add_node("plan", generate_plan)
graph.add_node("human_review", lambda s: s)  # ì‚¬ëŒ ìŠ¹ì¸ ë…¸ë“œ
graph.add_node("execute", execute_plan)

graph.set_entry_point("plan")
graph.add_edge("plan", "human_review")
graph.add_edge("human_review", "execute")
graph.add_edge("execute", END)

# interrupt_beforeë¡œ ì¤‘ë‹¨ì  ì„¤ì •
memory = MemorySaver()
app = graph.compile(
    checkpointer=memory,
    interrupt_before=["human_review"]
)

# ì‹¤í–‰
config = {"configurable": {"thread_id": "approval-1"}}
result = app.invoke({"request": "Delete all data"}, config=config)

# ìƒíƒœ í™•ì¸
state = app.get_state(config)
print(state.values["approval"])  # ê³„íš í™•ì¸

# ì‚¬ëŒì´ ìŠ¹ì¸ í›„ ì¬ê°œ
app.invoke(None, config=config)  # ê³„ì† ì‹¤í–‰
```

**í”Œë¡œìš°:**
```
START
  â”‚
  â–¼
Generate Plan
  â”‚
  â–¼
[INTERRUPT] â† ì‹¤í–‰ ì¤‘ë‹¨, ì‚¬ëŒ í™•ì¸ ëŒ€ê¸°
  â”‚
(ì‚¬ëŒ ìŠ¹ì¸)
  â”‚
  â–¼
Execute Plan
  â”‚
  â–¼
END
```

### 2. Streaming

**ì‹¤ì‹œê°„ ì¶œë ¥:**

```python
# ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
for event in app.stream({"query": "Hello"}):
    print(event)
    # {"retrieve": {"documents": [...]}}
    # {"generate": {"answer": "..."}}

# ë…¸ë“œë³„ ìŠ¤íŠ¸ë¦¬ë°
for node, output in app.stream(input, stream_mode="updates"):
    print(f"[{node}] {output}")
```

### 3. Parallel Execution (Send API)

**ë™ì ìœ¼ë¡œ ë³‘ë ¬ ë…¸ë“œ ìƒì„±:**

```python
from langgraph.constants import Send

def fanout(state):
    tasks = ["task1", "task2", "task3"]
    return [Send("worker", {"task": t}) for t in tasks]

def worker(state):
    result = process(state["task"])
    return {"results": [result]}

graph = StateGraph(State)
graph.add_node("fanout", fanout)
graph.add_node("worker", worker)
graph.add_node("aggregate", aggregate)

graph.set_entry_point("fanout")
graph.add_edge("worker", "aggregate")
graph.add_edge("aggregate", END)

app = graph.compile()
```

**ì‹¤í–‰:**
```
  fanout
    â”‚
 [Send API]
    â”‚
â”Œâ”€â”€â”€â”¼â”€â”€â”€â”
â”‚   â”‚   â”‚
â–¼   â–¼   â–¼
W1  W2  W3  (ë³‘ë ¬)
â”‚   â”‚   â”‚
â””â”€â”€â”€â”¼â”€â”€â”€â”˜
    â”‚
aggregate
```

### ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LangGraph Application                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         StateGraph                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”        â”‚  â”‚
â”‚  â”‚  â”‚Nodeâ”‚â†’â”‚Nodeâ”‚â†’â”‚Nodeâ”‚â†’â”‚Nodeâ”‚â†’END      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”€â”˜        â”‚  â”‚
â”‚  â”‚                     â”‚                    â”‚  â”‚
â”‚  â”‚                     â””â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚                            â”‚             â”‚  â”‚
â”‚  â”‚                     [Conditional Edge]   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Checkpointer (MemorySaver)       â”‚  â”‚
â”‚  â”‚  - ìƒíƒœ ì €ì¥/ë³µì›                         â”‚  â”‚
â”‚  â”‚  - Human-in-the-Loop                    â”‚  â”‚
â”‚  â”‚  - Time Travel                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Streaming                        â”‚  â”‚
â”‚  â”‚  - ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì¶œë ¥                     â”‚  â”‚
â”‚  â”‚  - ë…¸ë“œë³„ ì§„í–‰ ìƒí™©                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Parallel Execution (Send API)    â”‚  â”‚
â”‚  â”‚  - ë™ì  ë…¸ë“œ ìƒì„±                         â”‚  â”‚
â”‚  â”‚  - Map-Reduce íŒ¨í„´                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ® Part 8: ì‹¤ì „ í™œìš© ì‚¬ë¡€ (Unity ê²Œì„)

### 1. Dynamic NPC Dialogue System

**ìš”êµ¬ì‚¬í•­:** NPCê°€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ì–µí•˜ë©° ìƒí™©ì— ë§ê²Œ ì‘ë‹µ

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

class NPCState(TypedDict):
    player_name: str
    dialogue_history: Annotated[list, add_messages]
    npc_mood: str  # happy, angry, neutral
    quest_status: str

def npc_respond(state):
    context = f"""
    Player: {state['player_name']}
    Mood: {state['npc_mood']}
    Quest: {state['quest_status']}
    """
    prompt = f"{context}\nDialogue: {state['dialogue_history'][-1].content}"
    response = llm.invoke(prompt)

    return {"dialogue_history": [response]}

def update_mood(state):
    # ëŒ€í™” ë‚´ìš© ë¶„ì„í•´ì„œ ê¸°ë¶„ ë³€í™”
    last_message = state["dialogue_history"][-1].content
    if "rude" in last_message.lower():
        return {"npc_mood": "angry"}
    return {"npc_mood": "happy"}

graph = StateGraph(NPCState)
graph.add_node("respond", npc_respond)
graph.add_node("update_mood", update_mood)

graph.set_entry_point("respond")
graph.add_edge("respond", "update_mood")
graph.add_edge("update_mood", END)

memory = MemorySaver()
app = graph.compile(checkpointer=memory)

# Unityì—ì„œ í˜¸ì¶œ
config = {"configurable": {"thread_id": "npc-blacksmith-001"}}
result = app.invoke({
    "player_name": "Hero",
    "dialogue_history": [HumanMessage(content="ë¬´ê¸° íŒ”ì•„?")],
    "npc_mood": "neutral",
    "quest_status": "not_started"
}, config=config)
```

**ì›Œí¬í”Œë¡œìš°:**
```
Player Input
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NPC Respond â”‚  "ì–´ì„œì˜¤ì„¸ìš”! ë¬´ê¸°ëŠ” ì´ìª½ì…ë‹ˆë‹¤"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Update Mood  â”‚  mood = "happy"
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  [Checkpoint Save]
       â”‚
       â–¼
  Unity Display
```

### 2. Dynamic Quest Generator

**ìš”êµ¬ì‚¬í•­:** í”Œë ˆì´ì–´ ë ˆë²¨/ì„ í˜¸ë„ì— ë§ëŠ” í€˜ìŠ¤íŠ¸ ìƒì„±

```python
class QuestState(TypedDict):
    player_level: int
    preferences: list[str]  # ["combat", "exploration", "puzzle"]
    generated_quests: list[dict]
    selected_quest: dict

def generate_quest_ideas(state):
    prompt = f"""
    Player Level: {state['player_level']}
    Preferences: {state['preferences']}

    Generate 3 quest ideas.
    """
    ideas = llm.invoke(prompt)
    return {"generated_quests": parse_quests(ideas)}

def evaluate_quests(state):
    # ê° í€˜ìŠ¤íŠ¸ì˜ ì í•©ë„ í‰ê°€
    scores = []
    for quest in state["generated_quests"]:
        score = calculate_fit(quest, state["preferences"])
        scores.append(score)

    best_idx = scores.index(max(scores))
    return {"selected_quest": state["generated_quests"][best_idx]}

def refine_quest(state):
    quest = state["selected_quest"]
    refined = llm.invoke(f"Add details to quest: {quest}")
    return {"selected_quest": refined}

graph = StateGraph(QuestState)
graph.add_node("generate", generate_quest_ideas)
graph.add_node("evaluate", evaluate_quests)
graph.add_node("refine", refine_quest)

graph.set_entry_point("generate")
graph.add_edge("generate", "evaluate")
graph.add_edge("evaluate", "refine")
graph.add_edge("refine", END)

app = graph.compile()
```

**ì›Œí¬í”Œë¡œìš°:**
```
Player Profile
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate    â”‚  3ê°œ í€˜ìŠ¤íŠ¸ ì•„ì´ë””ì–´
â”‚  Quest Ideas â”‚  - ê³ ë¸”ë¦° í† ë²Œ (combat)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - ë˜ì „ íƒí—˜ (exploration)
       â”‚          - ìˆ˜ìˆ˜ê»˜ë¼ (puzzle)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evaluate    â”‚  preferences: ["combat"]
â”‚   Quests     â”‚  â†’ ê³ ë¸”ë¦° í† ë²Œ ì„ íƒ
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Refine      â”‚  ìƒì„¸ ì •ë³´ ì¶”ê°€:
â”‚   Quest      â”‚  - ìœ„ì¹˜: ì–´ë‘ìš´ ìˆ²
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - ë³´ìƒ: 100 gold, ê²½í—˜ì¹˜
       â”‚          - ë‚œì´ë„: ë ˆë²¨ 5 ê¶Œì¥
       â–¼
Unity Quest System
```

### 3. Adaptive Difficulty Adjustment

**ìš”êµ¬ì‚¬í•­:** í”Œë ˆì´ì–´ ì‹¤ë ¥ì— ë§ì¶° ì‹¤ì‹œê°„ ë‚œì´ë„ ì¡°ì •

```python
class DifficultyState(TypedDict):
    player_stats: dict  # {"hp": 100, "damage": 50, ...}
    recent_deaths: int
    win_rate: float
    current_difficulty: float
    adjustment: str

def analyze_performance(state):
    # ìµœê·¼ ì„±ê³¼ ë¶„ì„
    if state["win_rate"] < 0.3:
        return {"adjustment": "decrease"}
    elif state["win_rate"] > 0.7:
        return {"adjustment": "increase"}
    return {"adjustment": "maintain"}

def calculate_difficulty(state):
    current = state["current_difficulty"]

    if state["adjustment"] == "decrease":
        new_difficulty = max(0.5, current - 0.1)
    elif state["adjustment"] == "increase":
        new_difficulty = min(2.0, current + 0.1)
    else:
        new_difficulty = current

    return {"current_difficulty": new_difficulty}

def apply_to_game(state):
    difficulty = state["current_difficulty"]
    # Unityë¡œ ì „ì†¡í•  íŒŒë¼ë¯¸í„° ìƒì„±
    params = {
        "enemy_health_multiplier": difficulty,
        "enemy_damage_multiplier": difficulty,
        "spawn_rate_multiplier": difficulty
    }
    return {"game_params": params}

graph = StateGraph(DifficultyState)
graph.add_node("analyze", analyze_performance)
graph.add_node("calculate", calculate_difficulty)
graph.add_node("apply", apply_to_game)

graph.set_entry_point("analyze")
graph.add_edge("analyze", "calculate")
graph.add_edge("calculate", "apply")
graph.add_edge("apply", END)

app = graph.compile()

# Unityì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œ (ì˜ˆ: ë§¤ ì›¨ì´ë¸Œ ì¢…ë£Œ í›„)
result = app.invoke({
    "player_stats": {"hp": 80, "damage": 50},
    "recent_deaths": 5,
    "win_rate": 0.25,  # 25% ìŠ¹ë¥ 
    "current_difficulty": 1.0
})

print(result["current_difficulty"])  # 0.9 (ê°ì†Œ)
```

**ì›Œí¬í”Œë¡œìš°:**
```
Game Session Data
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Analyze     â”‚  win_rate = 25%
â”‚  Performance  â”‚  â†’ adjustment = "decrease"
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Calculate    â”‚  1.0 â†’ 0.9 (ë‚œì´ë„ ê°ì†Œ)
â”‚  Difficulty   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apply to     â”‚  enemy_health: Ã—0.9
â”‚  Game         â”‚  enemy_damage: Ã—0.9
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  spawn_rate: Ã—0.9
        â”‚
        â–¼
Unity Game Manager
     â”‚
     â–¼
Next Wave (ë” ì‰¬ì›Œì§)
```

---

## ğŸ“ Part 9: ì •ë¦¬

### LangGraph ì‚¬ìš© ì‹œì 

**âœ… LangGraph ì„ íƒ:**
- [ ] ìˆœí™˜ ì›Œí¬í”Œë¡œìš° í•„ìš” (ì¬ì‹œë„, ë£¨í”„)
- [ ] ë³µì¡í•œ ì¡°ê±´ë¶€ ë¡œì§ (if-elseê°€ ë§ìŒ)
- [ ] ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…
- [ ] Human-in-the-Loop í•„ìš”
- [ ] ìƒíƒœ ì²´í¬í¬ì¸íŠ¸ (ì¤‘ë‹¨/ì¬ê°œ)
- [ ] ë™ì  ì‘ì—… ë¶„ë°° (Orchestrator-Worker)

**ì˜ˆì‹œ ì½”ë“œ ë³µì¡ë„:**
```python
# LangGraphë¡œ êµ¬í˜„ (20ì¤„)
graph = StateGraph(State)
graph.add_node("step1", step1)
graph.add_conditional_edges("step1", router)
app = graph.compile()
```

### LangChain ì‚¬ìš© ì‹œì 

**âœ… LangChain ì„ íƒ:**
- [ ] ë‹¨ìˆœ ì„ í˜• ì²´ì¸ (A â†’ B â†’ C)
- [ ] RAG íŒŒì´í”„ë¼ì¸ (ê²€ìƒ‰ â†’ ìƒì„±)
- [ ] ì¼íšŒì„± ì‘ì—… (ë²ˆì—­, ìš”ì•½)
- [ ] ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
- [ ] ìƒíƒœ ê´€ë¦¬ ë¶ˆí•„ìš”

**ì˜ˆì‹œ ì½”ë“œ ë³µì¡ë„:**
```python
# LangChainìœ¼ë¡œ êµ¬í˜„ (5ì¤„)
chain = prompt | model | parser
result = chain.invoke(input)
```

### ì½”ë“œ ë¹„êµ

**ë™ì¼í•œ ì‘ì—… (RAG + ì¬ì‹œë„):**

```python
# LangChain (ì¬ì‹œë„ ìˆ˜ë™ êµ¬í˜„, 20ì¤„)
for i in range(3):
    docs = retriever.get_relevant_documents(query)
    answer = chain.invoke({"context": docs, "query": query})
    if validate(answer):
        break

# LangGraph (ì›Œí¬í”Œë¡œìš°ë¡œ í‘œí˜„, 10ì¤„)
graph = StateGraph(State)
graph.add_node("retrieve", retrieve)
graph.add_node("generate", generate)
graph.add_conditional_edges("generate", should_retry)
app = graph.compile()
```

### í•µì‹¬ ìš”ì•½

**LangChain:**
- ê°„ë‹¨í•œ ì²´ì´ë‹, RAG íŒŒì´í”„ë¼ì¸
- í”„ë¡œí† íƒ€ì´í•‘ì— ìµœì 
- ì½”ë“œ ê°„ê²°

**LangGraph:**
- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°, ë©€í‹° ì—ì´ì „íŠ¸
- í”„ë¡œë•ì…˜ ê¸°ëŠ¥ ë‚´ì¥ (ì²´í¬í¬ì¸íŠ¸, HITL)
- ìˆœí™˜ ê·¸ë˜í”„ë¡œ ìœ ì—°ì„± í™•ë³´

**ì„ íƒ ê¸°ì¤€:**
```
ë‹¨ìˆœ ì²´ì¸? â†’ LangChain
ìˆœí™˜ ë¡œì§? â†’ LangGraph
ë©€í‹° ì—ì´ì „íŠ¸? â†’ LangGraph
RAGë§Œ? â†’ LangChain
ìƒíƒœ ì¶”ì ? â†’ LangGraph
```

---

## ğŸ”— ì°¸ê³  ìë£Œ

- [LangChain ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/)
- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)
- [LangSmith](https://smith.langchain.com/)
- [GitHub - LangChain](https://github.com/langchain-ai/langchain)
- [GitHub - LangGraph](https://github.com/langchain-ai/langgraph)

---
