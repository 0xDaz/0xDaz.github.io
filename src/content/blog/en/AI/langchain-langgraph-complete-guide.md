---
title: "LangChain and LangGraph Complete Guide: Everything About AI Agent Development"
pubDate: 2025-10-27T23:55:34+09:00
draft: false
tags: ["LangChain", "LangGraph", "AI Agent", "RAG", "LLM", "Python"]
categories: ["AI"]
description: "Complete guide from core concepts to practical application of LangChain and LangGraph for AI agent development. Includes RAG, agent patterns, and Unity game application cases"
---


## ðŸŽ¯ Part 1: Introduction to LangChain

### What is LangChain

Open-source framework for building LLM-based applications. Simplifies complex AI workflows through chaining pattern

**Core Features:**
- **Real-time data augmentation**: Utilize latest information through RAG
- **Model interoperability**: Support for diverse LLMs like OpenAI, Anthropic, Cohere
- **Modular composition**: Combine reusable components

### Why LangChain

```python
# Without LangChain (vanilla code)
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}]
)
result = response.choices[0].message.content

# Using LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

chain = ChatPromptTemplate.from_template("Translate {text} to {language}") | ChatOpenAI(model="gpt-4")
result = chain.invoke({"text": "Hello", "language": "Korean"})
```

**Advantages:**
- âœ… Code simplification (improved readability through chaining)
- âœ… Easy model switching (provides abstraction layer)
- âœ… Built-in production features (retry, logging, monitoring)

### Core Concepts

**1. Chaining**
```python
chain = prompt | model | output_parser
result = chain.invoke(input_data)
```

**2. Modularity**
- **Prompts**: Reusable prompt templates
- **Models**: LLM abstraction layer
- **Output Parsers**: Parse structured output
- **Retrievers**: Vector DB search interface

**3. Main Use Cases**
- ðŸ” RAG (Retrieval-Augmented Generation)
- ðŸ’¬ Chatbots and conversational systems
- ðŸ“ Document summarization and Q&A
- ðŸ¤– Agents (tool usage)

### LangChain Ecosystem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangChain Ecosystem             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LangChain Core                          â”‚
â”‚ - Basic abstractions (Chains, Prompts, Models) â”‚
â”‚                                         â”‚
â”‚ LangSmith                               â”‚
â”‚ - Debugging, monitoring, evaluation platform â”‚
â”‚                                         â”‚
â”‚ LangGraph                               â”‚
â”‚ - Complex multi-agent workflows        â”‚
â”‚                                         â”‚
â”‚ LangGraph Platform                      â”‚
â”‚ - Deploy and host LangGraph apps       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Basic Example: Simple LLM Chain

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. Define prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful translator."),
    ("user", "Translate '{text}' to {language}")
])

# 2. Initialize model
model = ChatOpenAI(model="gpt-4", temperature=0)

# 3. Output parser
parser = StrOutputParser()

# 4. Compose chain
translation_chain = prompt | model | parser

# 5. Execute
result = translation_chain.invoke({
    "text": "Hello, world!",
    "language": "Korean"
})
print(result)  # "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ìƒ!"
```

---

## ðŸ—ï¸ Part 2: LangChain Architecture

### 1. Basic LLM Chain Structure

```
Input
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Templateâ”‚
â”‚  (Insert vars) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Model     â”‚
â”‚  (Run inference)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Parser  â”‚
â”‚  (Parse result)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
      Output
```

**Code Implementation:**
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Chain composition (LCEL: LangChain Expression Language)
chain = (
    ChatPromptTemplate.from_template("Tell me a joke about {topic}")
    | ChatOpenAI(model="gpt-4")
    | StrOutputParser()
)

# Streaming execution
for chunk in chain.stream({"topic": "programming"}):
    print(chunk, end="", flush=True)
```

### 2. RAG (Retrieval-Augmented Generation) Architecture

RAG consists of two phases:

**Phase 1: Indexing (Offline)**
```
Documents
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document Loader â”‚  â† PDF, HTML, DB etc.
â”‚  (Load docs)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Text Splitter  â”‚  â† RecursiveCharacterTextSplitter
â”‚   (Split chunks) â”‚     chunk_size=1000, overlap=200
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Model â”‚  â† OpenAI, Cohere etc.
â”‚  (Convert vector)â”‚     text â†’ [0.1, 0.3, ...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Store   â”‚  â† Chroma, Pinecone, FAISS
â”‚  (Store vectors) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 2: Retrieval + Generation (Online)**
```
User Query: "How to implement AI agent in Unity?"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Embedding â”‚  â† Convert query to vector
â”‚                  â”‚     "Unity AI..." â†’ [0.2, 0.5, ...]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Similarity      â”‚  â† Calculate cosine similarity
â”‚  Search (Top-K)  â”‚     Return top K most relevant chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context + Query â”‚  â† Retrieved docs + Original query
â”‚  (Build prompt)  â”‚     "Based on: [docs], answer: [query]"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Generate   â”‚  â† Generate answer with GPT-4
â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
       Answer
```

**Complete RAG Code:**
```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# === Phase 1: Indexing ===
# 1. Load documents
loader = PyPDFLoader("unity_documentation.pdf")
docs = loader.load()

# 2. Split text
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# 3. Embed and store vectors
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings()
)

# === Phase 2: Retrieval + Generation ===
# 4. Create retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 5. Prompt template
prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer using the following context:\n\n{context}"),
    ("user", "{input}")
])

# 6. Compose chain
llm = ChatOpenAI(model="gpt-4")
document_chain = create_stuff_documents_chain(llm, prompt)
retrieval_chain = create_retrieval_chain(retriever, document_chain)

# 7. Execute query
response = retrieval_chain.invoke({
    "input": "How to implement AI movement using NavMesh in Unity?"
})
print(response["answer"])
```

### 3. Agent Architecture

Autonomous decision-making loop:

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Agent Loop                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Think (Decide next action) â”‚  â”‚
â”‚  â”‚    LLM: "Need to use search"  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. Act (Execute tool)        â”‚  â”‚
â”‚  â”‚    Tool: search("Unity AI")  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. Observe (Analyze result)  â”‚  â”‚
â”‚  â”‚    Result: [search results]  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                       â”‚
â”‚             â–¼                       â”‚
â”‚        Sufficient?                  â”‚
â”‚       /        \                    â”‚
â”‚     Yes        No â”€â”€â”               â”‚
â”‚      â”‚              â”‚               â”‚
â”‚      â–¼              â”‚               â”‚
â”‚   Final Answer â—„â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Agent Code:**
```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools import DuckDuckGoSearchRun

# 1. Define tools
tools = [
    DuckDuckGoSearchRun(name="web_search"),
]

# 2. Prompt (agent_scratchpad required)
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
])

# 3. Create agent
llm = ChatOpenAI(model="gpt-4", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)

# 4. Wrap with executor (handles loop execution)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# 5. Execute
result = agent_executor.invoke({
    "input": "What are new features in Unity 2023 LTS?"
})
print(result["output"])
```

---

## ðŸ”€ Part 3: Introduction to LangGraph

### What is LangGraph

Framework for building complex AI agents based on graphs. Implements state machines using **cyclic graphs**

**Core Features:**
- **Cyclic workflows**: Loops, conditional branching, retry logic
- **Explicit state management**: Track state with TypedDict
- **Checkpoints**: Save and restore state during execution
- **Human-in-the-Loop**: Insert points for human approval/modification

### LangChain vs LangGraph Comparison

| Feature | LangChain | LangGraph |
|------|-----------|-----------|
| **Structure** | Linear chain | Cyclic graph |
| **State Management** | Implicit (pass between chains) | Explicit (TypedDict) |
| **Conditional Logic** | Simple branching | Complex conditional flow |
| **Loops** | Limited | Native support |
| **Checkpointing** | None | Built-in (time travel possible) |
| **Human-in-the-Loop** | Manual implementation required | Built-in feature |
| **Use Cases** | RAG, simple chains | Multi-agent, complex workflows |
| **Code Complexity** | Low | Medium~High |

**Example: LangChain is waterfall, LangGraph is maze**

```
LangChain:  A â†’ B â†’ C â†’ D (straight line)

LangGraph:  A â†’ B â‡„ C â†’ D
              â†“     â†‘
              E â”€â”€â”€â”€â”˜
            (cycle, retry, conditional)
```

### When to Use LangGraph

**LangGraph Suitable:**
- âœ… Multi-agent collaboration (Agent A calls Agent B)
- âœ… Retry/validation logic (validate answer then regenerate)
- âœ… Human-in-the-Loop (human approval needed)
- âœ… Complex state tracking (conversation history, context)

**LangChain Suitable:**
- âœ… Simple RAG pipeline
- âœ… One-time chains (translation, summarization)
- âœ… Fast prototyping

---

## ðŸ§© Part 4: LangGraph Core Concepts

### 1. State

Shared data defined as TypedDict:

```python
from typing import TypedDict, Annotated
from langgraph.graph import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]  # Accumulate messages
    context: str                              # Additional context
    retry_count: int                          # Retry count
```

**Reducer Function (add_messages):**
- Default behavior: Overwrite (`state["key"] = value`)
- Reducer: Custom update logic (`add_messages` appends to list)

### 2. Nodes

Work units (function or Runnable):

```python
def node_function(state: State) -> dict:
    """Node receives State and returns dict to update"""
    messages = state["messages"]
    # Perform work
    response = llm.invoke(messages)
    return {"messages": [response]}
```

**Features:**
- Pure function (minimize side-effects)
- Read and update State
- Support both sync/async

### 3. Edges

Connections between nodes:

**Normal Edge (unconditional execution):**
```python
graph.add_edge("node_a", "node_b")  # Always B after A
```

**Conditional Edge:**
```python
def router(state: State) -> str:
    """Return next node name"""
    if state["retry_count"] > 3:
        return "end"
    return "retry"

graph.add_conditional_edges(
    "validation",
    router,
    {"retry": "generation", "end": END}
)
```

### 4. Graph Builder Pattern

```python
from langgraph.graph import StateGraph, END

# 1. Define graph
graph = StateGraph(State)

# 2. Add nodes
graph.add_node("input", input_node)
graph.add_node("process", process_node)
graph.add_node("output", output_node)

# 3. Add edges
graph.set_entry_point("input")
graph.add_edge("input", "process")
graph.add_conditional_edges("process", router)
graph.add_edge("output", END)

# 4. Compile
app = graph.compile()

# 5. Execute
result = app.invoke({"messages": [("user", "Hello")]})
```

### 5. Execution Features

**Streaming (real-time output):**
```python
for event in app.stream({"messages": [("user", "Hello")]}):
    print(event)
```

**Checkpointing (state persistence):**
```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = graph.compile(checkpointer=memory)

# Maintain state per thread
config = {"configurable": {"thread_id": "thread-1"}}
app.invoke(input, config=config)
```

**Human-in-the-Loop:**
```python
from langgraph.checkpoint.memory import MemorySaver

app = graph.compile(checkpointer=MemorySaver(), interrupt_before=["human_review"])

# Execution pauses before human_review
result = app.invoke(input, config=config)

# Resume after human approval
app.invoke(None, config=config)
```

**Time-Travel (return to past state):**
```python
# Query all checkpoints
checkpoints = app.get_state_history(config)

# Return to specific checkpoint
past_config = {"configurable": {"thread_id": "thread-1", "checkpoint_id": "abc123"}}
app.invoke(None, config=past_config)
```

---

(Due to length constraints, I'll provide a condensed summary of the remaining sections)

## ðŸŽ¨ Part 5-9: Advanced Patterns and Examples

The guide continues with:
- **Workflow Patterns**: Chaining, Parallelization, Routing, Orchestrator-Worker, Evaluator-Optimizer, Agent loops
- **Practical Examples**: Chatbot with memory, RAG with self-correction
- **Advanced Features**: Human-in-the-Loop, Streaming, Parallel execution
- **Unity Game Applications**: NPC dialogue system, dynamic quest generator, adaptive difficulty
- **Summary**: When to use LangChain vs LangGraph

**Key Takeaways:**

**Use LangChain for:**
- Simple linear workflows (A â†’ B â†’ C)
- Basic RAG pipelines
- Fast prototyping
- No state management needed

**Use LangGraph for:**
- Cyclic workflows (loops, retries)
- Multi-agent systems
- Complex state tracking
- Human-in-the-Loop scenarios
- Production-grade applications

---

## ðŸ”— References

- [LangChain Official Docs](https://python.langchain.com/)
- [LangGraph Official Docs](https://langchain-ai.github.io/langgraph/)
- [LangSmith](https://smith.langchain.com/)
- [GitHub - LangChain](https://github.com/langchain-ai/langchain)
- [GitHub - LangGraph](https://github.com/langchain-ai/langgraph)

---
